---
title: 'MM-SOC: Benchmarking Multimodal Large Language Models in Social Media Platforms'

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
  - admin
  - Minje Choi
  - Gaurav Verma
  - Jindong Wang
  - Srijan Kumar

# Author notes (optional)
author_notes:
  - 'Lead author'

date: '2024-01-01T00:00:00Z'
doi: '10.18653/v1/2024.findings-acl.370'

# Schedule page publish date (NOT publication's date).
publishDate: '2024-01-01T00:00:00Z'

# Publication type.
# Accepts a single type but formatted as a YAML list (for Hugo requirements).
# Enter a publication type from the CSL standard.
publication_types: ['paper-conference']

# Publication name and optional abbreviated publication name.
publication: "ACL (Findings) 2024"
publication_short: "ACL'24"

abstract: Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehensively understand the information. Multimodal Large Language Models (MLLMs) have shown promise in addressing these challenges, yet they struggle with accurately interpreting the intertwined multimodal cues in social media content. We introduce MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content.

# Summary. An optional shortened abstract.
summary: Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehensively understand the information. Multimodal...

tags:
  - Multimodal Learning
  - Large Language Models
  - Social Media Analysis
  - Benchmarking

# Display this page in the Featured widget?
featured: false

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: 'https://arxiv.org/abs/2402.14154'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: 'Model architecture and key components'
  focal_point: 'Smart'
  preview_only: false
  alt_text: 'Figure showing the main model architecture and workflow'

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---

## Abstract

Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehensively understand the information. Multimodal Large Language Models (MLLMs) have shown promise in addressing these challenges, yet they struggle with accurately interpreting the intertwined multimodal cues in social media content. We introduce MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content.

## Keywords

Multimodal Learning, Large Language Models, Social Media Analysis, Benchmarking

## Links

- [Arxiv](https://arxiv.org/abs/2402.14154)
- [Doi](10.18653/v1/2024.findings-acl.370)
